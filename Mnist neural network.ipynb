{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Loading the mnist Dataset\n",
    "(train_data, train_targets), (test_data, test_targets) = mnist.load_data()\n",
    "print(train_data.shape )\n",
    "print(train_targets.shape) \n",
    "print(test_data.shape )\n",
    "print(test_targets.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6000, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#Number of samples needed during validation\n",
    "num_validation_samples = 0.1*train_data.shape[0]\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "print(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of samples\n",
    "num_test_samples = train_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data to be between 0 and 1 as each pixel has a value between 0 and 255\n",
    "scaled_train_data=train_data/255\n",
    "scaled_test_data=test_data/255\n",
    "input_shape = scaled_train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the numpy arrays into one tensorflow dataswt with label\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((scaled_train_data, train_targets))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((scaled_test_data, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data\n",
    "BUFFER_SIZE = 10000#Neccesary as a large dataset(hyperparameter that can be fine tuned)\n",
    "\n",
    "shuffled_train_and_valida_data = train_dataset.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating validation and training data\n",
    "validation_data =shuffled_train_and_valida_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_valida_data.skip(num_validation_samples)\n",
    "\n",
    "BATCH_SIZE = 100 #Hyperparameter that can be changed to fine tune the algorithimn\n",
    "#1 would be the stochastic gradient descent\n",
    "\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)#Only used on training data since validation data is backpropogated\n",
    "validation_data = validation_data.batch(num_validation_samples)#Has to use validation data at once\n",
    "test_data = test_dataset.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlining the model\n",
    "input_size= 784\n",
    "output_size =10\n",
    "hidden_layer_1 = 392\n",
    "hidden_layer_2 = 150\n",
    "hidden_layer_3 = 50\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "                            tf.keras.layers.Dense(hidden_layer_1, activation='relu'),#Hyperparameter to play around with\n",
    "                            tf.keras.layers.Dense(hidden_layer_2, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_3, activation='tanh'),\n",
    "      \n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 540 steps, validate for 1 steps\n",
      "Epoch 1/7\n",
      "540/540 [==============================] - 3s 5ms/step - loss: 0.2423 - accuracy: 0.9294 - val_loss: 0.1069 - val_accuracy: 0.9685\n",
      "Epoch 2/7\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0908 - accuracy: 0.9730 - val_loss: 0.0787 - val_accuracy: 0.9745\n",
      "Epoch 3/7\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0583 - accuracy: 0.9823 - val_loss: 0.0651 - val_accuracy: 0.9813\n",
      "Epoch 4/7\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0439 - accuracy: 0.9861 - val_loss: 0.0472 - val_accuracy: 0.9867\n",
      "Epoch 5/7\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0315 - accuracy: 0.9904 - val_loss: 0.0384 - val_accuracy: 0.9863\n",
      "Epoch 6/7\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0268 - accuracy: 0.9917 - val_loss: 0.0302 - val_accuracy: 0.9903\n",
      "Epoch 7/7\n",
      "540/540 [==============================] - 2s 4ms/step - loss: 0.0228 - accuracy: 0.9928 - val_loss: 0.0287 - val_accuracy: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x201016c9b48>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 7\n",
    "\n",
    "#model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs,validation_targets),verbose = 2)\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=validation_data,validation_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0809 - accuracy: 0.9771\n"
     ]
    }
   ],
   "source": [
    "#Test model\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.71%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {0:.2f}. Test accuracy: {1:.2f}%\".format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('First_attempt_at_Mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
